{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0b47d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import plotly.express as px\n",
    "import pprint\n",
    "import warnings \n",
    "# pyright: ignore\n",
    "warnings.filterwarnings(\"ignore\")   \n",
    "import os\n",
    "\n",
    "df = pd.read_csv(r'C:\\DECISIONTREESIMPLEMENTATION\\DECISIONTREESIMPLEMENTATION\\breast_cancer_data.csv')\n",
    "\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1504b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape\", df.shape)\n",
    "print(\"-\"*40)\n",
    "print(\"columns\", df.columns)\n",
    "print(\"-\"*40)\n",
    "print(df.info())\n",
    "print(\"-\"*40)\n",
    "print(\"Null value check\\n:\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf7d314",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dropping the redundant \\n\")\n",
    "df.drop(columns=['Unnamed: 32', 'id'],axis=1, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac76cd33",
   "metadata": {},
   "source": [
    "EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee50aedd",
   "metadata": {},
   "source": [
    "Markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a897acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6de96c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in the Output label\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['M', 'B'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Unique values in the Output label\\n\")\n",
    "df['diagnosis'].unique()\n",
    "# M : Malignant\n",
    "# B : Benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31005c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output label/ Target variable /Y-label ; data distribution\n",
    "#pie-plot ; proportion of M v/s B\n",
    "\n",
    "px.pie(df, \n",
    "       'diagnosis', \n",
    "       color ='diagnosis',\n",
    "       color_discrete_sequence= ['#007500', '#5CFF5C'],\n",
    "       title=\"Data Distribution \")\n",
    "\n",
    "\n",
    "\n",
    "# Inferences:\n",
    "# dataset is imbalanced(M:B= 63:37)\n",
    "# there are more cases of benign than malignant tumors\n",
    "# for imbalanced datasets, accuracy can be a misleading metric\n",
    "# for example, if 90% of the cases are  benign, the model will always predict benign \n",
    "# in such cases, we nwwd \"Balanced Accuracy\" metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a6c0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nbformat>=4.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9e3908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visually compare the features of each feature\n",
    "# FOR MALIGNENT TUMORS versus BENIGN TUMORS\n",
    "# for a given feature,  do its values tend to be diff for malignent tumors v/s benign tumors\n",
    "\n",
    "for colmn in df.drop('diagnosis', axis = 1).columns[:5]:\n",
    "    # for loop auto iterates over the first 5 feature columns in the dataframe\n",
    "    fig = px.box(data_frame=df,\n",
    "                 x='diagnosis',\n",
    "                 color='diagnosis',\n",
    "                 y=colmn,\n",
    "                 color_discrete_sequence= ['#007500', '#5CFF5C'],\n",
    "                 orientation='v')\n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181d697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for colmn in df.drop('diagnosis', axis = 1).columns[5:10]:\n",
    "    # for loop auto iterates over the first 5 feature columns in the dataframe\n",
    "    fig = px.scatter(data_frame=df,\n",
    "                 x = colmn,\n",
    "                 color='diagnosis',\n",
    "                 color_discrete_sequence= ['#007500', '#5CFF5C'],\n",
    "                 orientation='v')\n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc940b76",
   "metadata": {},
   "source": [
    "# Creating correlation with the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc07adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagnosis : M or B : Categorical\n",
    "# encode : 1 or 0: categorical\n",
    "df['diagnosis'] = (df['diagnosis'] == 'M').astype(int)\n",
    "# this line converts the categorical feature into numerical feature\n",
    "\n",
    "# setting M=1 and B=0\n",
    "# take the coreation\n",
    "corr = df.corr()\n",
    "plt.figure(figsize = (20,20))\n",
    "# heatmap\n",
    "sns.heatmap(corr, cmap='viridis_r', annot=True)\n",
    "plt.show()\n",
    "\n",
    "# coreation -1 to 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fb4efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1711bb",
   "metadata": {},
   "source": [
    "Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e055f3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we  should now choose which features  are  good enough predictors to be used to train the model\n",
    "# get the absolute corealation\n",
    "\n",
    "cor_target = abs(corr[\"diagnosis\"] )\n",
    " \n",
    " # select better coreation features\n",
    " # this is the filtering step\n",
    " # it creats a new list of relevent features\n",
    "relevant_features = cor_target[cor_target>0.25]\n",
    "\n",
    "# 0.25 is user defined threshold . it is the hyperparameter value\n",
    "# collect the name of features\n",
    "# list comprehension\n",
    " \n",
    "names = [index for index,value in relevant_features.items()]\n",
    " #Dtop the target variable from the results\n",
    " \n",
    "names.remove(\"diagnosis\")\n",
    " \n",
    "pprint.pprint(names)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812cd4f8",
   "metadata": {},
   "source": [
    "## Assign training data and training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ab998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[names].values\n",
    "y = df['diagnosis'].values.reshape(-1,1)\n",
    "\n",
    "# this line creates target vector or a target label\n",
    "# df['diagnosis'].values: (569,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25f4c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input features are:\",x.shape,\"output label shape :\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817c19f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to scale\n",
    "#standardization / z-score normalization\n",
    "# apply on x \n",
    "\n",
    "def scale(x):\n",
    "    '''\n",
    "    Parameters : numpy.ndarray)\n",
    "    \n",
    "    returns : numpy.ndarray\n",
    "    '''\n",
    "    \n",
    "    mean = np.mean(x, axis=0)\n",
    "    std = np.std(x, axis=0)\n",
    "    \n",
    "    # standardized this data\n",
    "    x = (x-mean)/std\n",
    "    \n",
    "    return x\n",
    "X = scale(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5b3167",
   "metadata": {},
   "source": [
    "## Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f2003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will start with all the examples at the root node\n",
    "# Then  we will calculate the information gain for each feature / gini index for each feature\n",
    "# then we will pick the feature with the highest information gain / lowest gini index\n",
    "# then we will split the data according to selected feature\n",
    "# we will repeat the process untill we reach the stopping criteria\n",
    "\n",
    "# Node class\n",
    "class Node:\n",
    "    '''\n",
    "    A class for a node in the decision tree.\n",
    "    '''\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, info_gain=None, value=None):\n",
    "        '''\n",
    "        Parameters:\n",
    "        -----------\n",
    "        feature : int\n",
    "            The index of the feature used for splitting the node.\n",
    "        threshold : float\n",
    "            The threshold value used for splitting the node.\n",
    "        left : Node\n",
    "            The left child node.\n",
    "        right : Node\n",
    "            The right child node.\n",
    "        info_gain : float\n",
    "            The information gain obtained by splitting the node.\n",
    "        value : int\n",
    "            The class label if the node is a leaf node.\n",
    "        '''\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.info_gain = info_gain\n",
    "        self.value = value\n",
    "\n",
    "# Explanation: \n",
    "# self.threshold and self.feature are used by Decision Nodes to store the question being asked at that node.\n",
    "# For example, \"is feature[i] <= 15.5?\"\n",
    "# self.right and self.left are used by both Decision Nodes and Leaf Nodes as pointers to child nodes. class\n",
    "class Node:\n",
    "    '''\n",
    "    A class for a node in the decision tree.\n",
    "    '''\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, info_gain=None, value=None):\n",
    "        '''\n",
    "        Parameters:\n",
    "        -----------\n",
    "        feature : int\n",
    "            The index of the feature used for splitting the node.\n",
    "        threshold : float\n",
    "            The threshold value used for splitting the node.\n",
    "        left : Node\n",
    "            The left child node.\n",
    "        right : Node\n",
    "            The right child node.\n",
    "        info_gain : float\n",
    "            The information gain obtained by splitting the node.\n",
    "        value : int\n",
    "            The class label if the node is a leaf node.\n",
    "        '''\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.info_gain = info_gain\n",
    "        self.value = value\n",
    "\n",
    "# Explanation: \n",
    "# self.threshold and self.feature \n",
    "# are used by Decision Nodes to store the question being asked at that node.\n",
    "# For example, \"is feature[i] <= 15.5?\"\n",
    "# self.right and self.left are used by both Decision Nodes and Leaf Nodes as pointers to child nodes.\n",
    "\n",
    "## self.value = value \n",
    "# used by leaf nodes to store the class label.\n",
    "# if a node is a final endpoint . it does not ask any question\n",
    "#it holds predicted class label or prediction for each branch\n",
    "#self.value will be 0 or 1 (malignent) for leaf nodes.\n",
    "\n",
    "\n",
    "# self.gain = gain\n",
    "# used by decision nodes to store the information gain obtained by splitting the node.\n",
    "# it helps to evaluate the quality of the split at that node.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d255ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Decision Tree\n",
    "\n",
    "class DecisionTree:\n",
    "    '''\n",
    "    This is a decision tree classifier.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,min_samples = 2 , max_depth = 3):\n",
    "        self.min_samples = min_samples\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "        \"We are setting hyper-parameters to control the growth of the tree prevent overfitting\"\n",
    "        \n",
    "    \n",
    "    def split_data(self, dataset,feature, threshold):\n",
    "        '''\n",
    "        Splits the given dataset based on the feature and threshold.\n",
    "        parameters:\n",
    "        - dataset: The dataset to split.\n",
    "        - feature  : Index of the feature to split on.\n",
    "        - threshold: The threshold value for the split.\n",
    "        \n",
    "        Returns : \n",
    "        left_dataset : subset of data with values less than or equal to the threshold\n",
    "        right_dataset : subset of data with values greater than the threshold\n",
    "        \n",
    "\n",
    "        '''\n",
    "        \n",
    "        # create empty arrays\n",
    "        left_dataset = []\n",
    "        right_dataset = []\n",
    "        \n",
    "        # loop through each row in the dataset in left and right basis the feature and threshold\n",
    "        \n",
    "        for row in dataset:\n",
    "            if row[feature] <= threshold:\n",
    "                left_dataset.append(row)\n",
    "            else:\n",
    "                right_dataset.append(row)\n",
    "                \n",
    "        # convert the left and right datasets into numpy arrays\n",
    "        left_dataset = np.array(left_dataset)\n",
    "        right_dataset = np.array(right_dataset)\n",
    "        \n",
    "        return left_dataset, right_dataset\n",
    "    \n",
    "    \n",
    "    \n",
    "    # write function to calculate Entropy\n",
    "    def entropy(self, y):\n",
    "        '''\n",
    "        Computes the entropy for given labels\n",
    "        Entropy suggests impurity or disorder in the dataset.\n",
    "\n",
    "        \n",
    "        Returns : float : Entropy value\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        entropy = 0.0\n",
    "        # this initializes the entropy to zero\n",
    "        \n",
    "        # use numpy's unique function to get the unique labels in y\n",
    "        labels = np.unique(y)\n",
    "        \n",
    "        for label in labels:\n",
    "            # find examples in y that have the current label\n",
    "            label_examples = y[y == label]\n",
    "            # Calculate the ratio of current label in y\n",
    "            pl = len(label_examples) / len(y)\n",
    "            # calculate the entropy for the current label and ratio \n",
    "            entropy += -pl * np.log2(pl) \n",
    "            \n",
    "            return entropy\n",
    "        \n",
    "    \n",
    "    # write function to calculate Gini Index/Information Gain\n",
    "    \n",
    "    def information_gain(self,parent,left,right):\n",
    "        '''\n",
    "        Computes the information gain from splitting the parent dataset into two datasets\n",
    "        Parameters:\n",
    "        parent(ndarray) : Input parent dataset\n",
    "        left : subset of parent dataset after the split on the feature\n",
    "        right : subset of parent dataset after the split on the feature\n",
    "        \n",
    "        Returns : \n",
    "        Information Gain on the split: float\n",
    "        '''\n",
    "        \n",
    "        # intiialize the information gain to zero\n",
    "        information_gain = 0.0\n",
    "        # compute the entropy of the parent dataset\n",
    "        parent_entropy = self.entropy(parent)\n",
    "        # calculate the weights for left and right datasets/nodes\n",
    "        weight_left = len(left) / len(parent)\n",
    "        weight_right = len(right) / len(parent)\n",
    "        # compute the entropy of the left and right datasets/nodes\n",
    "        entropy_left,entropy_right = self.entropy(left) , self.entropy(right)\n",
    "        # calculate the weighted entropy \n",
    "        # weighted_entropy = post split impurity\n",
    "        # parent entropy= pre split impurity \n",
    "        weighted_entropy = (weight_left * entropy_left) + (weight_right * entropy_right)\n",
    "        # calculate the information gain\n",
    "        information_gain = parent_entropy - weighted_entropy\n",
    "        \n",
    "        return information_gain\n",
    "    \n",
    "    \n",
    "    # function to get the best split\n",
    "    def best_split(self, dataset, num_samples, num_features):\n",
    "        \"\"\"\n",
    "        Finds the best split for the given dataset.\n",
    "\n",
    "        Args:\n",
    "        dataset (ndarray): The dataset to split.\n",
    "        num_samples (int): The number of samples in the dataset.\n",
    "        num_features (int): The number of features in the dataset.\n",
    "\n",
    "        Returns:\n",
    "        dict: A dictionary with the best split feature index, threshold, gain,\n",
    "              left and right datasets.\n",
    "        \"\"\"\n",
    "        # dictionary to store the best split values\n",
    "        best_split = {'gain':- 1, 'feature': None, 'threshold': None}\n",
    "        # loop over all the features\n",
    "        for feature_index in range(num_features):\n",
    "            #get the feature at the current feature_index\n",
    "            feature_values = dataset[:, feature_index] # column2 \n",
    "            #get unique values of that feature\n",
    "            thresholds = np.unique(feature_values)\n",
    "            # loop over all values of the feature\n",
    "            for threshold in thresholds:\n",
    "                # get left and right datasets\n",
    "                left_dataset, right_dataset = self.split_data(dataset, feature_index, threshold)\n",
    "                # check if either datasets is empty\n",
    "                if len(left_dataset) and len(right_dataset):\n",
    "                    # get y values of the parent and left, right nodes\n",
    "                    y, left_y, right_y = dataset[:, -1], left_dataset[:, -1], right_dataset[:, -1]\n",
    "                    # compute information gain based on the y values\n",
    "                    information_gain = self.information_gain(y, left_y, right_y)\n",
    "                    # update the best split if conditions are met\n",
    "                    if information_gain > best_split[\"gain\"]:\n",
    "                        best_split[\"feature\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"left_dataset\"] = left_dataset\n",
    "                        best_split[\"right_dataset\"] = right_dataset\n",
    "                        best_split[\"gain\"] = information_gain\n",
    "        return best_split\n",
    "    \n",
    "    \n",
    "    \n",
    "    def calculate_leaf_value(self, y):\n",
    "        \"\"\"\n",
    "        Calculates the most occurring value in the given list of y values.\n",
    "\n",
    "        Args:\n",
    "            y (list): The list of y values.\n",
    "\n",
    "        Returns:\n",
    "            The most occurring value in the list.\n",
    "        \"\"\"\n",
    "        y = list(y)\n",
    "        #get the highest present class in the array\n",
    "        most_occuring_value = max(y, key=y.count) # B : 0 , M : 1\n",
    "        return most_occuring_value\n",
    "    \n",
    "    \n",
    "    def build_tree(self, dataset, current_depth=0):\n",
    "        \"\"\"\n",
    "        Recursively builds a decision tree from the given dataset.\n",
    "\n",
    "        Args:\n",
    "        dataset (ndarray): The dataset to build the tree from.\n",
    "        current_depth (int): The current depth of the tree.\n",
    "\n",
    "        Returns:\n",
    "        Node: The root node of the built decision tree.\n",
    "        \"\"\"\n",
    "        # split the dataset into X, y values\n",
    "        X, y = dataset[:, :-1], dataset[:, -1]\n",
    "        n_samples, n_features = X.shape\n",
    "        # keeps spliting until stopping conditions are met\n",
    "        if n_samples >= self.min_samples and current_depth <= self.max_depth:\n",
    "            # Get the best split\n",
    "            best_split = self.best_split(dataset, n_samples, n_features)\n",
    "            # Check if gain isn't zero\n",
    "            if best_split[\"gain\"]:\n",
    "                # continue splitting the left and the right child. Increment current depth\n",
    "                left_node = self.build_tree(best_split[\"left_dataset\"], current_depth + 1)\n",
    "                right_node = self.build_tree(best_split[\"right_dataset\"], current_depth + 1)\n",
    "                # return decision node\n",
    "                return Node(best_split[\"feature\"], best_split[\"threshold\"],\n",
    "                            left_node, right_node, best_split[\"gain\"])\n",
    "\n",
    "        # compute leaf node value\n",
    "        leaf_value = self.calculate_leaf_value(y)\n",
    "        # return leaf node value\n",
    "        return Node(value=leaf_value)\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Builds and fits the decision tree to the given X and y values.\n",
    "\n",
    "        Args:\n",
    "        X (ndarray): The feature matrix.\n",
    "        y (ndarray): The target values.\n",
    "        \"\"\"\n",
    "        dataset = np.concatenate((X, y), axis=1)\n",
    "        self.root = self.build_tree(dataset)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the class labels for each instance in the feature matrix X.\n",
    "\n",
    "        Args:\n",
    "        X (ndarray): The feature matrix to make predictions for.\n",
    "\n",
    "        Returns:\n",
    "        list: A list of predicted class labels.\n",
    "        \"\"\"\n",
    "        # Create an empty list to store the predictions\n",
    "        predictions = []\n",
    "        # For each instance in X, make a prediction by traversing the tree\n",
    "        for x in X:\n",
    "            prediction = self.make_prediction(x, self.root)\n",
    "            # Append the prediction to the list of predictions\n",
    "            predictions.append(prediction)\n",
    "        # Convert the list to a numpy array and return it\n",
    "        np.array(predictions)\n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "    \n",
    "    def make_prediction(self, x, node):\n",
    "        \"\"\"\n",
    "        Traverses the decision tree to predict the target value for the given feature vector.\n",
    "\n",
    "        Args:\n",
    "        x (ndarray): The feature vector to predict the target value for.\n",
    "        node (Node): The current node being evaluated.\n",
    "\n",
    "        Returns:\n",
    "        The predicted target value for the given feature vector.\n",
    "        \"\"\"\n",
    "        # if the node has value i.e it's a leaf node extract it's value\n",
    "        if node.value != None:\n",
    "            return node.value\n",
    "        else:\n",
    "            #if it's node a leaf node we'll get it's feature and traverse through the tree accordingly\n",
    "            feature = x[node.feature]\n",
    "            if feature <= node.threshold:\n",
    "                return self.make_prediction(x, node.left)\n",
    "            else:\n",
    "                return self.make_prediction(x, node.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3755a3",
   "metadata": {},
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280b02a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "# X_train,y_train, X_test,y_test = train_test_split(X, y, random_state=41, test_size=0.2)\n",
    "\n",
    "def train_test_split(X, y, random_state=41, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Splits the data into training and testing sets.\n",
    "\n",
    "    Parameters:\n",
    "        X (numpy.ndarray): Features array of shape (n_samples, n_features).\n",
    "        y (numpy.ndarray): Target array of shape (n_samples,).\n",
    "        random_state (int): Seed for the random number generator. Default is 42.\n",
    "        test_size (float): Proportion of samples to include in the test set. Default is 0.2.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[numpy.ndarray]: A tuple containing X_train, X_test, y_train, y_test.\n",
    "    \"\"\"\n",
    "    # Get number of samples\n",
    "    n_samples = X.shape[0] # rows are samples\n",
    "\n",
    "    # Set the seed for the random number generator\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Shuffle the indices\n",
    "    shuffled_indices = np.random.permutation(np.arange(n_samples))\n",
    "\n",
    "    # Determine the size of the test set\n",
    "    test_size = int(n_samples * test_size)\n",
    "\n",
    "    # Split the indices into test and train\n",
    "    test_indices = shuffled_indices[:test_size]\n",
    "    train_indices = shuffled_indices[test_size:]\n",
    "\n",
    "    # Split the features and target arrays into test and train\n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f847345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of a classification model.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "        y_true (numpy array): A numpy array of true labels for each data point.\n",
    "        y_pred (numpy array): A numpy array of predicted labels for each data point.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "        float: The accuracy of the model\n",
    "    \"\"\"\n",
    "    y_true = y_true.flatten()\n",
    "    total_samples = len(y_true)\n",
    "    correct_predictions = np.sum(y_true == y_pred) # i want the total number where the actual and prediction is exactly the same\n",
    "    return (correct_predictions / total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e6ec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_accuracy(y_true, y_pred):\n",
    "    \"\"\"Calculate the balanced accuracy for a multi-class classification problem.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        y_true (numpy array): A numpy array of true labels for each data point.\n",
    "        y_pred (numpy array): A numpy array of predicted labels for each data point.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        balanced_acc : The balanced accuracyof the model\n",
    "\n",
    "    \"\"\"\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_true = y_true.flatten()\n",
    "    # Get the number of classes\n",
    "    n_classes = len(np.unique(y_true))\n",
    "\n",
    "    # Initialize an array to store the sensitivity and specificity for each class\n",
    "    sen = []\n",
    "    spec = []\n",
    "    # Loop over each class\n",
    "    for i in range(n_classes):\n",
    "        # Create a mask for the true and predicted values for class i\n",
    "        mask_true = y_true == i\n",
    "        mask_pred = y_pred == i\n",
    "\n",
    "        # Calculate the true positive, true negative, false positive, and false negative values\n",
    "        TP = np.sum(mask_true & mask_pred)\n",
    "        TN = np.sum((mask_true != True) & (mask_pred != True))\n",
    "        FP = np.sum((mask_true != True) & mask_pred)\n",
    "        FN = np.sum(mask_true & (mask_pred != True))\n",
    "\n",
    "        # Calculate the sensitivity (true positive rate) and specificity (true negative rate)\n",
    "        sensitivity = TP / (TP + FN)\n",
    "        specificity = TN / (TN + FP)\n",
    "\n",
    "        # Store the sensitivity and specificity for class i\n",
    "        sen.append(sensitivity)\n",
    "        spec.append(specificity)\n",
    "    # Calculate the balanced accuracy as the average of the sensitivity and specificity for each class\n",
    "    average_sen =  np.mean(sen)\n",
    "    average_spec =  np.mean(spec)\n",
    "    balanced_acc = (average_sen + average_spec) / n_classes\n",
    "\n",
    "    return balanced_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c38b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=41, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9211f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn implementation \n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Create a decision tree classifier model object.\n",
    "decision_tree_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Train the decision tree classifier model using the training data.\n",
    "decision_tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained model to make predictions on the test data.\n",
    "predictions = decision_tree_classifier.predict(X_test)\n",
    "\n",
    "# Calculate evaluating metrics\n",
    "print(f\" Model's Accuracy: {accuracy(y_test, predictions)}\")\n",
    "print(f\"Model's Balanced Accuracy: {balanced_accuracy(y_test, predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81de3371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report \n",
    "# confusion matrix\n",
    "# TP,TN,FP,FN\n",
    "# precision , recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f378188c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1441f1c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6b87c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd35b6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37de7db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a94a189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906a7fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50bf6c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f8f6e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6e0b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e20ea9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d486437a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ba0939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1165ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
